from sentence_transformers import SentenceTransformer, util, models, losses
from sklearn.model_selection import train_test_split
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
import datasets  # pip install datasets
import nltk
import re
from sentence_transformers import SentenceTransformer, models
from sentence_transformers.datasets import DenoisingAutoEncoderDataset
from torch.utils.data import DataLoader
from sentence_transformers.losses import DenoisingAutoEncoderLoss
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers import InputExample


class TestModels:
    abstract = [
        'Ripples are brief high-frequency electrographic events with important roles in episodic memory. However, the in vivo circuit mechanisms coordinating ripple-related activity among local and distant neuronal ensembles are not well understood. Here, we define key characteristics of a long-distance projecting GABAergic cell group in the mouse hippocampus that selectively exhibits high-frequency firing during ripples while staying largely silent during theta-associated states when most other GABAergic cells are active. The high ripple-associated firing commenced before ripple onset and reached its maximum before ripple peak, with the signature theta-OFF, ripple-ON firing pattern being preserved across awake and sleep states. Controlled by septal GABAergic, cholinergic, and CA3 glutamatergic inputs, these ripple-selective cells innervate parvalbumin and cholecystokinin-expressing local interneurons while also targeting a variety of extra-hippocampal regions. These results demonstrate the existence of a hippocampal GABAergic circuit element that is uniquely positioned to coordinate ripple-related neuronal dynamics across neuronal assemblies.']
    abstract_like = [
        'Sharp wave ripples (SWRs) are high-frequency synchronization events generated by hippocampal neuronal circuits during various forms of learning and reactivated during memory consolidation and recall. There is mounting evidence that SWRs are essential for storing spatial and social memories in rodents and short-term episodic memories in humans. Sharp wave ripples originate mainly from the hippocampal CA3 and subiculum, and can be transmitted to modulate neuronal activity in cortical and subcortical regions for long-term memory consolidation and behavioral guidance. Different hippocampal subregions have distinct functions in learning and memory. For instance, the dorsal CA1 is critical for spatial navigation, episodic memory, and learning, while the ventral CA1 and dorsal CA2 may work cooperatively to store and consolidate social memories. Here, we summarize recent studies demonstrating that SWRs are essential for the consolidation of spatial, episodic, and social memories in various hippocampal-cortical pathways, and review evidence that SWR dysregulation contributes to cognitive impairments in neurodegenerative and neurodevelopmental diseases']
    abstract_not_like = [
        'Memories are believed to be encoded by sparse ensembles of neurons in the brain. However, it remains unclear whether there is functional heterogeneity within individual memory engrams, i.e., if separate neuronal subpopulations encode distinct aspects of the memory and drive memory expression differently. Here, we show that contextual fear memory engrams in the mouse dentate gyrus contain functionally distinct neuronal ensembles, genetically defined by the Fos- or Npas4-dependent transcriptional pathways. The Fos-dependent ensemble promotes memory generalization and receives enhanced excitatory synaptic inputs from the medial entorhinal cortex, which we find itself also mediates generalization. The Npas4-dependent ensemble promotes memory discrimination and receives enhanced inhibitory drive from local cholecystokinin-expressing interneurons, the activity of which is required for discrimination. Our study provides causal evidence for functional heterogeneity within the memory engram and reveals synaptic and circuit mechanisms used by each ensemble to regulate the memory discrimination-generalization balance.']
    abstract_not_at_all_like = [
        'This article describes neural models of attention. Since attention is not a disembodied process, the article explains how brain processes of consciousness, learning, expectation, attention, resonance, and synchrony interact. These processes show how attention plays a critical role in dynamically stabilizing perceptual and cognitive learning throughout our lives. Classical concepts of object and spatial attention are replaced by mechanistically precise processes of prototype, boundary, and surface attention. Adaptive resonances trigger learning of bottom-up recognition categories and top-down expectations that help to classify our experiences, and focus prototype attention upon the patterns of critical features that predict behavioral success. These feature-category resonances also maintain the stability of these learned memories. Different types of resonances induce functionally distinct conscious experiences during seeing, hearing, feeling, and knowing that are described and explained, along with their different attentional and anatomical correlates within different parts of the cerebral cortex. All parts of the cerebral cortex are organized into layered circuits. Laminar computing models show how attention is embodied within a canonical laminar neocortical circuit design that integrates bottom-up filtering, horizontal grouping, and top-down attentive matching. Spatial and motor processes obey matching and learning laws that are computationally complementary to those obeyed by perceptual and cognitive processes. Their laws adapt to bodily changes throughout life, and do not support attention or conscious states.']

    def calculate_cosine_score(self, model_name, first_keywords, second_keywords):
        model = SentenceTransformer(model_name)

        embeddings_1 = model.encode(first_keywords, convert_to_numpy=True, normalize_embeddings=True)
        embeddings_2 = model.encode(second_keywords, convert_to_numpy=True, normalize_embeddings=True)

        cosine_score = util.pytorch_cos_sim(embeddings_1, embeddings_2)

        return cosine_score

    def calculate_cosine_score_doc2vec(self, first_keywords, second_keywords):
        doc2vec_model = Doc2Vec.load("doc2vec_model")

        tokenized_doc1 = word_tokenize(first_keywords.lower())
        tokenized_doc2 = word_tokenize(second_keywords.lower())

        doc2vec_embedding1 = doc2vec_model.infer_vector(tokenized_doc1)
        doc2vec_embedding2 = doc2vec_model.infer_vector(tokenized_doc2)

        cosine_score = util.pytorch_cos_sim([doc2vec_embedding1], [doc2vec_embedding2])

        return cosine_score

    def calculate_weight(self, cosine_score):
        return 1 / ((sum(sum(cosine_score.numpy())) + 1))

    def print_cosine_weight(self, model_name, first_keywords, second_keywords):
        cosine_score = self.calculate_cosine_score(model_name, first_keywords, second_keywords)
        print('cosine score:', cosine_score)
        print('weight:', self.calculate_weight(cosine_score))
        print('-----------')

    def print_model_cosine_weight(self, model_name):
        print('model: ' + model_name)
        print('abstract - abstract_like')
        self.print_cosine_weight(model_name, self.abstract_like, self.abstract)
        print('abstract - abstract_not_like')
        self.print_cosine_weight(model_name, self.abstract_not_like, self.abstract)
        print('abstract_like - abstract_not_like')
        self.print_cosine_weight(model_name, self.abstract_like, self.abstract_not_like)
        print('abstract_like - abstract_not_at_all_like')
        self.print_cosine_weight(model_name, self.abstract_like, self.abstract_not_at_all_like)
        print('\n')

    def compare_embedding_similarity(self):
        sts = datasets.load_dataset('glue', 'stsb', split='validation')

        # normalize the 0 -> 5 range
        sts = sts.map(lambda x: {'label': x['label'] / 5.0})

        samples = []
        for sample in sts:
            # reformat to use InputExample
            samples.append(InputExample(
                texts=[sample['sentence1'], sample['sentence2']],
                label=sample['label']
            ))

        evaluator = EmbeddingSimilarityEvaluator.from_input_examples(
            samples, write_csv=False
        )

        model = SentenceTransformer('output/tsdae-bert-base-uncased')
        evaluator(model)

    def main(self):
        model_1 = 'output/tsdae-bert-base-uncased'
        model_2 = 'paraphrase-MiniLM-L12-v2'

        self.print_model_cosine_weight(model_1)
        self.print_model_cosine_weight(model_2)

        #self.compare_embedding_similarity()

        # print(self.calculate_cosine_score_doc2vec(self.abstract, self.abstract_like))


if __name__ == "__main__":
    test = TestModels()
    test.main()
